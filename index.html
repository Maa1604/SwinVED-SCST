<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="description"
			content="A novel Vision Encoder-Decoder (VED) model for Difference Medical VQA effectively compares chest X-rays, identifying significant changes like pneumonia with state-of-the-art accuracy, enhancing clinical decision-making.">
	<meta name="keywords" content="Nerfies, D-NeRF, NeRF">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMA-MedVQA: A Medical Visual Question Answering Dataset Constructed with LLaMA-based Annotations</title>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
	<script>
		window.dataLayer = window.dataLayer || [];

		function gtag() {
		dataLayer.push(arguments);
		}

		gtag('js', new Date());

		gtag('config', 'G-PYVRSFMDRL');
	</script>

	<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
	<link rel="stylesheet" href="./static/css/bulma.min.css">
	<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
	<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
	<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="./static/css/index.css">
	<link rel="icon" href="./static/images/prhlt.png">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script defer src="./static/js/fontawesome.all.min.js"></script>
	<script src="./static/js/bulma-carousel.min.js"></script>
	<script src="./static/js/bulma-slider.min.js"></script>
	<script src="./static/js/index.js"></script>
</head>

<body>
<section class="hero">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title">LLaMA-MedVQA: A Medical Visual Question Answering Dataset Constructed with LLaMA-based Annotations</h1>
					<div class="is-size-5 publication-authors">
						<span class="author-block">
						<a href=".">Mohamed Aas-Alas</a><sup>*</sup>,
						</span>
						<span class="author-block">
						<a href=".">Miquel Obrador-Reina</a><sup>*</sup>,</span>
						<span class="author-block">
						<a href=".">Luis-Jesus Marhuenda</a><sup>*</sup>,</span>
						<span class="author-block">
						<a href=".">Alberto Albiol</a>,
						</span>
						<span class="author-block">
						<a href=".">Roberto Paredes</a>,
						</span>
					</div>

					<div class="is-size-5 publication-authors">
						<!-- <span class="author-block">Universitat Politècnica de València<br>MIDL 2025</span> -->
						<span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
					</div>

					<div class="column has-text-centered">
						<div class="publication-links">
						<!-- PDF Link. -->
						<span class="link-block">
							<a href="https://ljmtendero.github.io/A-VED-Model-For-Difference-Medical-VQA/static/pdfs/paper.pdf"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fas fa-file-pdf"></i>
							</span>
							<span>Paper</span>
							</a>
						</span>
						<!-- <span class="link-block">
							<a href="https://openreview.net/pdf?id=8CNssOg7fk"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fas fa-file-pdf"></i>
							</span>
							<span>OpenReview</span>
							</a>
						</span> -->
						<!-- Video Link. -->
						<!-- <span class="link-block">
							<a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fab fa-youtube"></i>
							</span>
							<span>Video</span>
							</a>
						</span> -->
						<!-- Code Link. -->
						<span class="link-block">
							<a href="https://github.com/Maa1604/llama-vqa/tree/main"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="fab fa-github"></i>
							</span>
							<span>Code</span>
							</a>
						</span>
						<!-- Dataset Link. -->
						 <span class="link-block">
							<a href="https://physionet.org/content/mimic-cxr/2.1.0/" class="external-link button is-normal is-rounded is-dark">
								<span class="icon">
									<i class="far fa-images"></i>
								</span>
								<span>MIMIC-CXR Dataset</span>
							</a>
						</span>
						<span class="link-block">
							<a href="https://physionet.org/content/medical-diff-vqa/1.0.1/" class="external-link button is-normal is-rounded is-dark">
								<span class="icon">
									<i class="fas fa-file-csv"></i>
								</span>
								<span>LLaMA-MedVQA Dataset</span>
							</a>
						</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Abstract</h2>
				<div class="content has-text-justified">
					<p>
						The interpretation of chest X-rays (CXRs) is a critical yet time-consuming task in clinical radiology, often limited by the availability of expert radiologists. To address this challenge, we introduce a novel large-scale medical Visual Question Answering (VQA) dataset derived from the MIMIC-CXR database, containing over 3.4 million question-answer (QA) pairs across 15 clinically relevant categories. Unlike prior datasets, our QA pairs are generated using a large language model (LLaMA 3.1) guided by a carefully crafted prompt structure to produce rich, nuanced, and evidence-based textual answers grounded in radiology reports. We address limitations of existing datasets—such as templated responses and linguistic monotony—by ensuring diversity, completeness, and clinical fidelity in our QA pairs. We also introduce a baseline Vision Encoder-Decoder model composed of a Swin Transformer image encoder and a 3-layer transformer decoder, trained through a multi-stage strategy that includes supervised learning and reinforcement learning (SCST) with BERTScore rewards. Extensive experiments demonstrate the effectiveness of our approach across multiple evaluation metrics, establishing a strong benchmark for future research in medical VQA. Our dataset and baseline model pave the way for building clinically meaningful AI tools that can assist radiologists by answering complex diagnostic questions with accuracy and interpretability.
					</p>
				</div>
			</div>
		</div>
	</div>
</section>
<!-- End paper abstract -->

<!-- Architecture -->
<!-- <section class="section hero">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Architecture</h2>
				<div class="publication-image">
					<img src="./static/images/architecture.jpg" alt="Architecture of the proposed model" class="hover-zoom">
				</div>
			</div>
		</div>
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<div class="content has-text-justified">
					<p>
						The proposed architecture focuses on a Vision Encoder-Decoder scheme specifically designed to compare pairs of chest X-ray images and generate answers about changes between them. The <b>visual component</b> uses a Transformer-based backbone pre-trained and fine-tuned on radiograph data to extract representations from both images. These representations are augmented with a learnable indicator that signals which image is the reference and which is the current one, so that the fusion process can explicitly distinguish between the two inputs.<br><br>
						
						The <b>text decoder</b> is a lightweight Transformer that processes the tokenized question and integrates the differentiated visual information via cross-attention. Given the relatively limited vocabulary and structure of questions in this domain, a smaller decoder is chosen to generate precise answers without requiring large-scale pre-trained language models. At each generation step, the decoder combines the textual context (the question and previously generated tokens) with the fused visual features, producing the answer autoregressively and ensuring coherence in comparing the reference and current images.<br><br>
						
						The <b>training strategy</b> is organized in staged phases: first, the visual encoder is fine-tuned exclusively on radiographs to learn domain-specific features; next, the text decoder is integrated and initially trained alone with the encoder frozen, optimizing visual-text fusion in isolation; finally, a joint fine-tuning of both encoder and decoder is performed to adjust the entire system. During this process, techniques such as hard example selection and image augmentations are applied to strengthen the model’s robustness against real variations in radiographs.
					</p>
				</div>
			</div>
		</div>
	</div>
</section> -->
<!-- End architecture -->

<!-- Dataset -->
<section class="section hero">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Dataset</h2>
				<div class="publication-image">
					<img src="./static/images/FinalLabelDistribution.png" alt="Final Label Distribution" class="hover-zoom">
				</div>
			</div>
		</div>
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<div class="content has-text-justified">
					<p>
						In this study, we introduce a novel Visual Question Answering (VQA) dataset derived from the MIMIC-CXR dataset <i>(Johnson et al., 2019)</i>, a publicly available collection of 227,835 chest radiograph studies, each accompanied by a detailed radiology report and corresponding images. Our processed dataset encompasses a total of <b>377,110 chest X-ray images</b> and is tailored for the development of clinically meaningful VQA systems.<br><br>
						
						The dataset features <b>3,487,542 question-answer (QA) pairs</b> distributed across <b>15 clinically relevant categories</b>: Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Mediastinum, Fracture, Lung Lesion, Lung Opacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, Support Devices, and Heart-related conditions. This makes it one of the most comprehensive datasets available for medical VQA research.<br><br>
						
						Each QA pair is generated based on expert-derived question templates applied to structured findings and free-text reports, ensuring both diversity and medical validity. The image above visualizes the final distribution of QA pairs across the 15 diagnostic categories, highlighting the dataset’s clinical balance and coverage. This resource enables the training and evaluation of models capable of answering detailed diagnostic queries from radiographic images.
					</p>
				</div>
			</div>
		</div>
	</div>
</section>
<!-- End dataset -->


<!-- Results -->
<!-- <section class="section hero">
	<div class="container is-max-desktop">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3">Results</h2>
				<div class="publication-image">
					<img src="./static/images/results.svg" alt="Results of the proposed model" class="hover-zoom">
				</div>
			</div>
		</div> -->
		<!-- <div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<div class="content has-text-justified">
					<p>
						[Escribe aquí sobre los resultados]
					</p>
				</div>
			</div>
		</div> -->
	<!-- </div>
</section> -->
<!-- End architecture -->

<!-- Stage Comparison Table -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
		<div class="content has-text-justified">
          <p>
            We implemented a Vision Encoder-Decoder (VED) model designed for medical Visual Question Answering (VQA) on chest radiographs. To optimize performance, we trained the model using a structured three-stage approach:
            <br><br>
            <strong>Stage 1:</strong> The text decoder is trained while keeping the visual encoder (Swin Transformer) frozen. This allows the decoder to learn effective language generation based on static visual features.<br>
            <strong>Stage 2:</strong> The visual encoder is unfrozen, enabling joint training with the decoder. This improves feature alignment between the visual and textual modalities.<br>
            <strong>Stage 3:</strong> Reinforcement Learning (RL) is applied to fine-tune the model using task-specific metrics, leading to further improvements in answer relevance and fluency.
          </p>
        </div>
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Model</th>
                <th>BLEU-1</th>
                <th>BLEU-2</th>
                <th>BLEU-3</th>
                <th>BLEU-4</th>
                <th>METEOR</th>
                <th>ROUGE-L</th>
                <th>CIDEr</th>
                <th>BERTScore</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Train Decoder FT-Swin</td>
                <td>0.2837</td>
                <td>0.2132</td>
                <td>0.1669</td>
                <td>0.1323</td>
                <td>0.1707</td>
                <td>0.4125</td>
                <td>1.6421</td>
                <td>0.5937</td>
              </tr>
              <tr>
                <td>+ Unfreeze Swin</td>
                <td>0.2867</td>
                <td>0.2165</td>
                <td>0.17</td>
                <td>0.1352</td>
                <td>0.1724</td>
                <td>0.4137</td>
                <td><strong>1.6514</strong></td>
                <td>0.5954</td>
              </tr>
              <tr>
                <td>+ RL</td>
                <td><strong>0.3046</strong></td>
                <td><strong>0.2290</strong></td>
                <td><strong>0.1790</strong></td>
                <td><strong>0.1416</strong></td>
                <td><strong>0.1756</strong></td>
                <td><strong>0.4143</strong></td>
                <td>1.6329</td>
                <td><strong>0.5957</strong></td>
              </tr>
            </tbody>
          </table>
          <p class="has-text-centered mt-4"><em>Table: Comparison of our model performance through all the stages. The best results are shown in <strong>bold</strong>.</em></p>
        </div>
		<div class="content has-text-justified mt-5">
          <p>
            We invite fellow researchers to explore our newly released dataset and benchmark their models on this challenging medical VQA task. With over 3 million QA pairs and clinically grounded question types, the dataset provides a rich testbed for innovation. We encourage the community to propose new architectures or training strategies that can further improve the metrics reported above.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Stage Comparison Table -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
	<h2 class="title">BibTeX</h2>
	<pre><code>@inproceedings{obrador-reina2025unveiling,
  title      = {Unveiling Differences: A Vision Encoder-Decoder Model for Difference Medical Visual Question Answering},
  author     = {Luis-Jesus Marhuenda and Miquel Obrador-Reina and Mohamed Aas-Alas and Alberto Albiol and Roberto Paredes},
  booktitle  = {Medical Imaging with Deep Learning},
  year       = {2025},
  url        = {https://openreview.net/forum?id=8CNssOg7fk}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
	<div class="content has-text-centered">
	  <a class="icon-link" href="https://github.com/lightved-prhlt" class="external-link" disabled>
		<i class="fab fa-github"></i>
	  </a>
	</div>
	<div class="columns is-centered">
	  <div class="column is-8">
		<div class="content has-text-centered">
			<p>Template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, thanks!</p>
		</div>
	  </div>
	</div>
  </div>
</footer>

</body>
</html>
